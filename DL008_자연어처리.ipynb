{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL008_자연어처리.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ca-gY8Cr32UN","colab_type":"text"},"source":["## 단어의 원핫 인코딩"]},{"cell_type":"code","metadata":{"id":"symF0Qxg1Q6j","colab_type":"code","colab":{}},"source":["# 사용될 문장 설정 / 변수 초기화\n","import numpy as np\n","\n","samples = ['AI is too difficult', 'AI is very complicated']\n","#samples = ['나는 학생입니다', '학생은 학교에 갑니다']\n","token_index = {}\n","max_length = 8;    #최대 토큰 수 (최대 단어 수)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nb2BcMes4Dkk","colab_type":"text"},"source":["### 토큰을 분리하고 인덱스를 할당 (인덱스를 1부터 시작)\n"]},{"cell_type":"code","metadata":{"id":"fRedVndf4GAI","colab_type":"code","colab":{}},"source":["for sample in samples:\n","    for word in sample.split():\n","        # 만약 token_index에 word가 없다면 인덱스를 할당 (0은 사용하지 않음)\n","        if word not in token_index:\n","            token_index[word] = len(token_index) + 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JU64impD4NaJ","colab_type":"code","outputId":"5e784605-5584-4346-eaff-33496f1b14bd","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1563330265646,"user_tz":-540,"elapsed":649,"user":{"displayName":"솔빈권","photoUrl":"https://lh6.googleusercontent.com/-DXetUgU0uEc/AAAAAAAAAAI/AAAAAAAAACs/jktii9RvF7I/s64/photo.jpg","userId":"06049098615445897991"}}},"source":["## 문장 분리\n","for sample in samples:\n","   print(sample)   "],"execution_count":3,"outputs":[{"output_type":"stream","text":["AI is too difficult\n","AI is very complicated\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TwDqH8dz4cPi","colab_type":"code","outputId":"a1889ea4-d804-49bd-9d7e-5eeb7bd1ac50","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1563330579237,"user_tz":-540,"elapsed":629,"user":{"displayName":"솔빈권","photoUrl":"https://lh6.googleusercontent.com/-DXetUgU0uEc/AAAAAAAAAAI/AAAAAAAAACs/jktii9RvF7I/s64/photo.jpg","userId":"06049098615445897991"}}},"source":["## 단어 분리\n","for word in sample.split():\n","   print(word)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["AI\n","is\n","very\n","complicated\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5w3QWUJC4qBa","colab_type":"code","outputId":"47ed8c5a-a0cc-4a01-82f6-fd53558a49a6","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1563330579530,"user_tz":-540,"elapsed":445,"user":{"displayName":"솔빈권","photoUrl":"https://lh6.googleusercontent.com/-DXetUgU0uEc/AAAAAAAAAAI/AAAAAAAAACs/jktii9RvF7I/s64/photo.jpg","userId":"06049098615445897991"}}},"source":["## 토큰의 수 출력\n","token_index[word] = len(token_index) + 1\n","print(token_index[word])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vzredYpW5Bto","colab_type":"code","outputId":"595b2067-76d7-4b12-c50f-126d5834e410","colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"status":"ok","timestamp":1563330580090,"user_tz":-540,"elapsed":583,"user":{"displayName":"솔빈권","photoUrl":"https://lh6.googleusercontent.com/-DXetUgU0uEc/AAAAAAAAAAI/AAAAAAAAACs/jktii9RvF7I/s64/photo.jpg","userId":"06049098615445897991"}}},"source":["# 원핫인코딩 결과를 저장할 변수 초기화\n","results = np.zeros(shape=(len(samples), max_length, max(token_index.values()) + 1))\n","\n","results"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.]],\n","\n","       [[0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0.]]])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"AoVADr_p55kz","colab_type":"code","outputId":"5f2b95f5-4098-45b7-db9b-477f234d6381","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1563330587922,"user_tz":-540,"elapsed":603,"user":{"displayName":"솔빈권","photoUrl":"https://lh6.googleusercontent.com/-DXetUgU0uEc/AAAAAAAAAAI/AAAAAAAAACs/jktii9RvF7I/s64/photo.jpg","userId":"06049098615445897991"}}},"source":["# 원핫인코딩 \n","# 리스트, 튜플, 문자열의 자료형을 입력받아 인덱스 값을 포함하는 자료형으로 반환\n","for i, sample in enumerate(samples):\n","    for j, word in list(enumerate(sample.split()))[:max_length]:\n","        # token_index 에서 word의 인덱스를 가져온다\n","        index = token_index.get(word)\n","        # word에 일치하는 위치를 1. 으로 설정\n","        results[i, j, index] = 1.\n","\n","print(results[0, 1])\n","print(results[1, 1])\n","print(results[1, 3])\n","print(results[1, 7])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[0. 0. 1. 0. 0. 0. 0. 0.]\n","[0. 0. 1. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 1.]\n","[0. 0. 0. 0. 0. 0. 0. 0.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Dks1_H29-pH","colab_type":"text"},"source":["## 문자의 원핫인코딩"]},{"cell_type":"code","metadata":{"id":"ww8OLALl-C9C","colab_type":"code","colab":{}},"source":["import numpy as np\n","import string\n","\n","samples = ['AI is too difficult', 'AI is very complicated']\n","max_length = 50"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QBLcRzkc-WJh","colab_type":"code","outputId":"52683236-dfe9-411b-ccb7-dce0d2266f57","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1563330620375,"user_tz":-540,"elapsed":370,"user":{"displayName":"솔빈권","photoUrl":"https://lh6.googleusercontent.com/-DXetUgU0uEc/AAAAAAAAAAI/AAAAAAAAACs/jktii9RvF7I/s64/photo.jpg","userId":"06049098615445897991"}}},"source":["characters = string.printable\n","\n","characters"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"px6Rnaku_tKD","colab_type":"code","outputId":"efcac52b-39eb-40bf-e11c-d8dfb2bfa72c","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1563330623882,"user_tz":-540,"elapsed":581,"user":{"displayName":"솔빈권","photoUrl":"https://lh6.googleusercontent.com/-DXetUgU0uEc/AAAAAAAAAAI/AAAAAAAAACs/jktii9RvF7I/s64/photo.jpg","userId":"06049098615445897991"}}},"source":["token_index = dict(zip(characters, range(1, len(characters) + 1)))\n","\n","token_index"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'\\t': 96,\n"," '\\n': 97,\n"," '\\x0b': 99,\n"," '\\x0c': 100,\n"," '\\r': 98,\n"," ' ': 95,\n"," '!': 63,\n"," '\"': 64,\n"," '#': 65,\n"," '$': 66,\n"," '%': 67,\n"," '&': 68,\n"," \"'\": 69,\n"," '(': 70,\n"," ')': 71,\n"," '*': 72,\n"," '+': 73,\n"," ',': 74,\n"," '-': 75,\n"," '.': 76,\n"," '/': 77,\n"," '0': 1,\n"," '1': 2,\n"," '2': 3,\n"," '3': 4,\n"," '4': 5,\n"," '5': 6,\n"," '6': 7,\n"," '7': 8,\n"," '8': 9,\n"," '9': 10,\n"," ':': 78,\n"," ';': 79,\n"," '<': 80,\n"," '=': 81,\n"," '>': 82,\n"," '?': 83,\n"," '@': 84,\n"," 'A': 37,\n"," 'B': 38,\n"," 'C': 39,\n"," 'D': 40,\n"," 'E': 41,\n"," 'F': 42,\n"," 'G': 43,\n"," 'H': 44,\n"," 'I': 45,\n"," 'J': 46,\n"," 'K': 47,\n"," 'L': 48,\n"," 'M': 49,\n"," 'N': 50,\n"," 'O': 51,\n"," 'P': 52,\n"," 'Q': 53,\n"," 'R': 54,\n"," 'S': 55,\n"," 'T': 56,\n"," 'U': 57,\n"," 'V': 58,\n"," 'W': 59,\n"," 'X': 60,\n"," 'Y': 61,\n"," 'Z': 62,\n"," '[': 85,\n"," '\\\\': 86,\n"," ']': 87,\n"," '^': 88,\n"," '_': 89,\n"," '`': 90,\n"," 'a': 11,\n"," 'b': 12,\n"," 'c': 13,\n"," 'd': 14,\n"," 'e': 15,\n"," 'f': 16,\n"," 'g': 17,\n"," 'h': 18,\n"," 'i': 19,\n"," 'j': 20,\n"," 'k': 21,\n"," 'l': 22,\n"," 'm': 23,\n"," 'n': 24,\n"," 'o': 25,\n"," 'p': 26,\n"," 'q': 27,\n"," 'r': 28,\n"," 's': 29,\n"," 't': 30,\n"," 'u': 31,\n"," 'v': 32,\n"," 'w': 33,\n"," 'x': 34,\n"," 'y': 35,\n"," 'z': 36,\n"," '{': 91,\n"," '|': 92,\n"," '}': 93,\n"," '~': 94}"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"DjSFcfixGLEz","colab_type":"code","outputId":"bdcaafbc-d9fa-429a-d6e7-1a0e2f4923ea","colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"status":"ok","timestamp":1563330707897,"user_tz":-540,"elapsed":593,"user":{"displayName":"솔빈권","photoUrl":"https://lh6.googleusercontent.com/-DXetUgU0uEc/AAAAAAAAAAI/AAAAAAAAACs/jktii9RvF7I/s64/photo.jpg","userId":"06049098615445897991"}}},"source":["# 변수 초기화 (0으로 채움), 원-핫 인코딩 수행\n","results = np.zeros((len(samples), max_length, max(token_index.values())+1))\n","\n","# sample에 포함된 문자들의 ASCII 코드에 해당하는 위치를 1.으로 설정\n","for i, sample in enumerate(samples):\n","    for j, character in enumerate(sample):\n","        index = token_index.get(character)\n","        results[i, j, index] = 1.\n","\n","print(results[0, 1])\n","print(results[1, 1])\n","print(results[1, 3])\n","print(results[1, 16])\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lJsGMK8eCWDd","colab_type":"text"},"source":["## Tokenizer를 이용한 단어 분리, 빈도수 분석, 단어 인덱스 할당"]},{"cell_type":"code","metadata":{"id":"qKEYsvV2CWKb","colab_type":"code","outputId":"98382fff-a304-4435-a197-6bddb42f2079","colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["from keras.preprocessing.text import Tokenizer\n","\n","#가장 빈도가 높은 n-1개의 단어만 선택\n","tokenizer = Tokenizer(num_words=5)\n","#단어 인덱스 분석 (단어, 빈도수, 문서수, 인덱스)\n","tokenizer.fit_on_texts(samples)\n","# 단어의 빈도수를 저장하고 있는 dict 자료형\n","print(tokenizer.word_docs)\n","# 단어와 고유 인덱스를 저장하고 있는 dict 자료형 (빈도수 순서로 인덱싱)\n","print(tokenizer.word_index)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["defaultdict(<class 'int'>, {'too': 1, 'is': 2, 'difficult': 1, 'ai': 2, 'complicated': 1, 'very': 1})\n","{'ai': 1, 'is': 2, 'too': 3, 'difficult': 4, 'very': 5, 'complicated': 6}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ua-qqAuaB4MT","colab_type":"code","outputId":"478a398c-56da-4832-d86f-1622b9dc4220","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# 문자열을 정수 인덱스의 리스트로 변환 (설정 빈도수에 해당하는 문자만 변환)\n","sequences = tokenizer.texts_to_sequences(samples)\n","print(sequences)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[1, 2, 3, 4], [1, 2]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AY8gYl4CDCVs","colab_type":"text"},"source":["- very와 complicated는 빈도수가 5-6번째이므로 제외\n"]},{"cell_type":"code","metadata":{"id":"DFG5Z9aMDExk","colab_type":"code","outputId":"18d2c6f0-6a13-4619-c9e0-003acbe5238e","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# 이진 벡터 행렬로 변환 (binary : 단어 존재 여부, count : 단어 수, tfidf : 단어 빈도, freq : 단어 비율)\n","on_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n","print(on_hot_results)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0. 1. 1. 1. 1.]\n"," [0. 1. 1. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4v7GX8o-DfBD","colab_type":"text"},"source":["- 첫 번째 인덱스는 사용하지 않음"]},{"cell_type":"markdown","metadata":{"id":"b0dYT5-uEPO1","colab_type":"text"},"source":["# Embedding 층을 사용하여 단어 임베딩 학습하기"]},{"cell_type":"code","metadata":{"id":"6sC0PugCETpe","colab_type":"code","colab":{}},"source":["from keras.datasets import imdb\n","from keras import preprocessing\n","from keras.models import Sequential\n","from keras.layers import Flatten, Dense, Embedding\n","\n","# 특성으로 사용할 단어 수\n","max_features = 10000\n","# 사용할 텍스트의 최대 길이 (짧으면 0으로 채움, 길면 자름)\n","maxlen = 20"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0gxesfcBEYt7","colab_type":"text"},"source":["### 함수 로딩, 사용할 문장 설정, 데이터 변환"]},{"cell_type":"code","metadata":{"id":"c41H8gyMEY07","colab_type":"code","colab":{}},"source":["from keras.datasets import imdb\n","\n","# 영화 리뷰는 X_train에, 감성 정보는 y_train에 저장\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","\n","# 훈련 및 테스트 데이터 리스트를 2D 텐서로 변환\n","x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vDSdS3TMKNtv","colab_type":"text"},"source":["### IMDB 원본 전처리하기 (imdb.load_data()와 numpy 버전의 문제로 사용할 수 없는 경우)"]},{"cell_type":"code","metadata":{"id":"eJuC4HRMKjhQ","colab_type":"code","outputId":"30d072ef-f30d-4ad8-ab1f-89cc33788a4b","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"49tnXfLNKHA-","colab_type":"code","colab":{}},"source":["import os\n","\n","imdb_dir = \"/gdrive/My Drive/Colab Notebooks/인공지능강의안/data/aclImdb/\"\n","train_dir = os.path.join(imdb_dir, 'train')\n","\n","labels = []\n","texts = []\n","\n","for label_type in ['neg', 'pos']:\n","    dir_name = os.path.join(train_dir, label_type)\n","    for fname in os.listdir(dir_name):\n","        if fname[-4:] == '.txt':\n","            f = open(os.path.join(dir_name, fname), encoding='utf8')\n","            texts.append(f.read())\n","            f.close()\n","            if label_type == 'neg':\n","                labels.append(0)\n","            else:\n","                labels.append(1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nwRDeT-GLscm","colab_type":"text"},"source":["### 데이터 토큰화"]},{"cell_type":"code","metadata":{"id":"4BbIgGiqLsQa","colab_type":"code","outputId":"a21f5d9a-3cfc-4b3c-9ac2-e6c3f28262cd","colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","maxlen = 100  # 100개 단어 이후는 버림\n","training_samples = 200  # 훈련 샘플은 200개\n","validation_samples = 10000  # 검증 샘플은 10,000개\n","max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용\n","\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","word_index = tokenizer.word_index\n","print('%s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n","\n","data = pad_sequences(sequences, maxlen=maxlen)\n","\n","labels = np.asarray(labels)\n","print('데이터 텐서의 크기:', data.shape)\n","print('레이블 텐서의 크기:', labels.shape)\n","\n","# 데이터를 훈련 세트와 검증 세트로 분할.\n","# 샘플이 순서대로 있기 때문에 (부정 샘플이 모두 나온 후에 긍정 샘플이 옴) \n","# 먼저 데이터를 섞음.\n","indices = np.arange(data.shape[0])\n","np.random.shuffle(indices)\n","data = data[indices]\n","labels = labels[indices]\n","\n","x_train = data[:training_samples]\n","y_train = labels[:training_samples]\n","x_val = data[training_samples: training_samples + validation_samples]\n","y_val = labels[training_samples: training_samples + validation_samples]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0개의 고유한 토큰을 찾았습니다.\n","데이터 텐서의 크기: (0, 100)\n","레이블 텐서의 크기: (0,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0fu2uPnja7qJ","colab_type":"text"},"source":["### 모델 설정 및 실행"]},{"cell_type":"code","metadata":{"id":"OmRCqTPga6Dw","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Embedding(max_features, 8, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","\n","history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n","print(history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nSoom1UpbLsa","colab_type":"text"},"source":["## 사전에 훈련된 단어 임베딩 사용하기\n","\n","- 압축 해제한 파일(.txt 파일)을 파싱하여 단어(즉 문자열)와 이에 상응하는 벡터 표현(즉 숫자 벡터)를 매핑하는 인덱스를 만듬"]},{"cell_type":"code","metadata":{"id":"vGor1lPvbRYs","colab_type":"code","outputId":"4679b336-7523-427d-acd2-9da3060d8881","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["glove_dir = '/gdrive/My Drive/Colab Notebooks/인공지능강의안/data/'\n","\n","embeddings_index = {}\n","f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('%s개의 단어 벡터를 찾았습니다.' % len(embeddings_index))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["400000개의 단어 벡터를 찾았습니다.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"b1guj-R4c3Wx","colab_type":"text"},"source":["- Embedding 층에 주입할 수 있도록 임베딩 행렬을 만듬\n","- 이 행렬의 크기는 (max_words, embedding_dim)이어야 함\n","- 이 행렬의 i번째 원소는 (토큰화로 만든) 단어 인덱스의 i번째 단어에 상응하는 embedding_dim 차원 벡터임\n","- 인덱스 0은 어떤 단어나 토큰도 아닐 경우를 나타냄"]},{"cell_type":"code","metadata":{"id":"xipsrlG9c1gd","colab_type":"code","colab":{}},"source":["embedding_dim = 100\n","\n","embedding_matrix = np.zeros((max_words, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if i < max_words:\n","        if embedding_vector is not None:\n","            # 임베딩 인덱스에 없는 단어는 모두 0이 됨\n","            embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tf-0SxkbdKQy","colab_type":"text"},"source":["###  모델 설정"]},{"cell_type":"code","metadata":{"id":"aWErp5XzdMba","colab_type":"code","outputId":"f89b4701-da5c-4958-f14e-42604ad0a83d","colab":{"base_uri":"https://localhost:8080/","height":437}},"source":["from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","model = Sequential()\n","model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0716 20:32:52.872034 140268230244224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0716 20:32:52.897710 140268230244224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0716 20:32:52.903372 140268230244224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 100, 100)          1000000   \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 10000)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 32)                320032    \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 33        \n","=================================================================\n","Total params: 1,320,065\n","Trainable params: 1,320,065\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oZh2-STQdRWK","colab_type":"text"},"source":["### 모델에 GloVe 임베딩 로드하기"]},{"cell_type":"code","metadata":{"id":"eDpt_zlCdPuy","colab_type":"code","outputId":"5f4ce197-43ac-4543-fb77-069761b816e8","colab":{"base_uri":"https://localhost:8080/","height":110}},"source":["model.layers[0].set_weights([embedding_matrix])\n","# Embedding 층을 동결(trainable 속성을 False로 설정)\n","model.layers[0].trainable = False"],"execution_count":0,"outputs":[{"output_type":"stream","text":["W0716 20:32:57.059489 140268230244224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","W0716 20:32:57.061861 140268230244224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"dXYlv3VRdbCK","colab_type":"text"},"source":["### 훈련 평가"]},{"cell_type":"code","metadata":{"id":"hDxy_mp1dcxh","colab_type":"code","colab":{}},"source":["model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n","\n","model.save_weights('pre_trained_glove_model.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UxWRcDnhdh9q","colab_type":"text"},"source":["### 시각화"]},{"cell_type":"code","metadata":{"id":"eTylq5LgdjV1","colab_type":"code","colab":{}},"source":["acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0oFTq_AVd4Ng","colab_type":"text"},"source":["- 이 모델은 과대적합이 빠르게 시작 (훈련 샘플 수가 작기 때문)\n","\n","### 사전 훈련된 단어 임베딩을 사용하지 않거나 임베딩 층을 동결하지 않고 같은 모델을 훈련 수 있음"]},{"cell_type":"code","metadata":{"id":"IO9xXtHFeIqq","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","model = Sequential()\n","model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Mt78pP8eOck","colab_type":"code","colab":{}},"source":["acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WRDJbr9LeQzq","colab_type":"text"},"source":["- 검증 정확도는 50% 초반에 멈추어 있음 (사전 훈련된 단어 임베딩을 사용하는 것이 임베딩을 함께 훈련하는 것보다 나음)\n","\n","\n","### 훈련 샘플을 2,000개로 늘려서 테스트"]},{"cell_type":"code","metadata":{"id":"KmKQ9Ja0eZsK","colab_type":"code","colab":{}},"source":["training_samples = 2000\n","x_train = data[:training_samples]\n","y_train = labels[:training_samples]\n","x_val = data[training_samples: training_samples + validation_samples]\n","y_val = labels[training_samples: training_samples + validation_samples]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6DXMbp8NebXS","colab_type":"code","colab":{}},"source":["history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OeH29ofReenm","colab_type":"code","colab":{}},"source":["acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qd6bn3yQeguN","colab_type":"text"},"source":["- 훈련 샘플의 수를 늘리니 단어 임베딩을 같이 훈련하는 모델의 검증 정확도가 70%를 넘김."]},{"cell_type":"markdown","metadata":{"id":"3rzPylT3elS-","colab_type":"text"},"source":["### 평가하기"]},{"cell_type":"code","metadata":{"id":"ww6xBwUXenaT","colab_type":"code","colab":{}},"source":["test_dir = os.path.join(imdb_dir, 'test')\n","\n","labels = []\n","texts = []\n","\n","for label_type in ['neg', 'pos']:\n","    dir_name = os.path.join(test_dir, label_type)\n","    for fname in sorted(os.listdir(dir_name)):\n","        if fname[-4:] == '.txt':\n","            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n","            texts.append(f.read())\n","            f.close()\n","            if label_type == 'neg':\n","                labels.append(0)\n","            else:\n","                labels.append(1)\n","\n","sequences = tokenizer.texts_to_sequences(texts)\n","x_test = pad_sequences(sequences, maxlen=maxlen)\n","y_test = np.asarray(labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lIM1q8AJeq1V","colab_type":"code","colab":{}},"source":["### 첫번째 모델을 평가"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UdYktvdKepEV","colab_type":"code","colab":{}},"source":["model.load_weights('pre_trained_glove_model.h5')\n","model.evaluate(x_test, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"541pe7nvexPf","colab_type":"text"},"source":["- 테스트 정확도는 겨우 50% 정도 (훈련 샘플이 작아서)"]}]}